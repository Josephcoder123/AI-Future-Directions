{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMewPZ1TnfQlPkr410elKf3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5jWZcaY3tqD",
        "outputId": "b1a9adb6-3bbf-476c-ae48-c1eb2d36bbd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
            "\u001b[1m14536120/14536120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Predictions: [[('n04286575', 'spotlight', np.float32(0.27482948)), ('n03196217', 'digital_clock', np.float32(0.036674242)), ('n03729826', 'matchstick', np.float32(0.032014605))]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load a pre-trained lightweight model (e.g., MobileNet)\n",
        "model = tf.keras.applications.MobileNetV2(weights='imagenet')\n",
        "\n",
        "# Simulate input data (e.g., image from a camera)\n",
        "image = np.random.rand(224, 224, 3)  # Random image for demonstration\n",
        "image = tf.keras.applications.mobilenet_v2.preprocess_input(image[np.newaxis, ...])\n",
        "\n",
        "# Perform inference on the edge device\n",
        "predictions = model.predict(image)\n",
        "decoded_predictions = tf.keras.applications.mobilenet_v2.decode_predictions(predictions, top=3)\n",
        "print(\"Predictions:\", decoded_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class SmartThermostat:\n",
        "    def __init__(self):\n",
        "        self.preferred_temperature = 22  # Default preferred temperature\n",
        "\n",
        "    def learn_preferences(self, user_behavior):\n",
        "        # Simulate learning user preferences\n",
        "        self.preferred_temperature = sum(user_behavior) / len(user_behavior)\n",
        "\n",
        "    def adjust_temperature(self):\n",
        "        # Simulate temperature adjustment\n",
        "        current_temperature = random.randint(18, 26)\n",
        "        if current_temperature != self.preferred_temperature:\n",
        "            print(f\"Adjusting temperature to {self.preferred_temperature}°C\")\n",
        "        else:\n",
        "            print(\"Temperature is optimal.\")\n",
        "\n",
        "# Simulate user behavior (e.g., preferred temperatures over a week)\n",
        "user_behavior = [22, 23, 22, 21, 22, 23, 22]\n",
        "thermostat = SmartThermostat()\n",
        "thermostat.learn_preferences(user_behavior)\n",
        "thermostat.adjust_temperature()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgdWNnWp4N8K",
        "outputId": "bdeb3f6c-9fd0-4b7a-c0ed-7701e3a0826b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting temperature to 22.142857142857142°C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class HeartRateMonitor:\n",
        "    def __init__(self):\n",
        "        self.normal_heart_rate = range(60, 100)  # Normal heart rate range\n",
        "\n",
        "    def monitor(self):\n",
        "        # Simulate heart rate data\n",
        "        heart_rate = random.randint(50, 120)\n",
        "        if heart_rate not in self.normal_heart_rate:\n",
        "            print(f\"Alert: Abnormal heart rate detected ({heart_rate} BPM)!\")\n",
        "        else:\n",
        "            print(f\"Heart rate is normal ({heart_rate} BPM).\")\n",
        "\n",
        "# Simulate monitoring\n",
        "monitor = HeartRateMonitor()\n",
        "monitor.monitor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKQCupAH4pQx",
        "outputId": "7302f9e0-568c-4d0a-a802-4c203271e899"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heart rate is normal (65 BPM).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained text generation model\n",
        "generator = pipeline('text-generation', model='distilgpt2')\n",
        "\n",
        "# Generate content suggestions\n",
        "prompt = \"The future of AI in healthcare\"\n",
        "suggestions = generator(prompt, max_length=50, num_return_sequences=3)\n",
        "for i, suggestion in enumerate(suggestions):\n",
        "    print(f\"Suggestion {i+1}: {suggestion['generated_text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlLqlz4g5EkA",
        "outputId": "7f808eed-04b4-4ea4-cc8e-94e188e0f81f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suggestion 1: The future of AI in healthcare and healthcare will come down to the number of jobs and jobs created and how we deal with that.”\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The future of AI in healthcare and healthcare will come down to the number of jobs and jobs created and how we deal with that.”\n",
            "Suggestion 2: The future of AI in healthcare and medicine will require a huge amount of thought and study to be done and to see if there is meaningful progress in this area. I am working on this in the future and will be working with the AI community to be in touch with the AI community which is working on this project.\n",
            "\n",
            "\n",
            "If you would like to follow me on Twitter and Facebook, follow me on Twitter.\n",
            "I have also created the AI World: The Future of AI, a blog post that is updated every day at the end of this month. It is designed to provide information on how AI is changing the way people interact with real human beings. The blog posts are based on the current trends and trends from the AI community, and the information is made available on github.\n",
            "I am now working on an AI World: The Future of AI, a blog post that is updated every day at the end of this month. It is designed to provide information on how AI is changing the way people interact with real human beings. The blog posts are based on the current trends and trends from the AI community, and the information is made available on github.\n",
            "If you would like to follow me on Twitter and Facebook, follow me on Twitter.\n",
            "I have also created the AI World: The Future of\n",
            "Suggestion 3: The future of AI in healthcare, and how we can change healthcare infrastructure for the better.”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Simulate medical image data (e.g., pixel values)\n",
        "X = np.random.rand(100, 256)  # 100 images, 256 features each\n",
        "y = np.random.randint(0, 2, 100)  # Binary labels (0: healthy, 1: tumor)\n",
        "\n",
        "# Train a simple AI model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict on new data\n",
        "new_image = np.random.rand(1, 256)\n",
        "prediction = model.predict(new_image)\n",
        "print(\"Tumor detected!\" if prediction[0] == 1 else \"No tumor detected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-6yf61X6Z2V",
        "outputId": "d5de5aec-1c04-43ed-b49f-fdba01a379e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tumor detected!\n"
          ]
        }
      ]
    }
  ]
}